{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importamos las librerías necesarias :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from io import BytesIO\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Guardamos las imágenes con sus respectivas clases en \"complete_df\" :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = './Imagenes/Trayectorias_comprimidas/longitud_28/raw-img.zip'\n",
    "\n",
    "complete_df = pd.DataFrame()\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    file_list = [f for f in zip_ref.namelist() if f.startswith('raw-img/') and f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    complete_df['file_name'] = pd.Series(file_list)\n",
    "    complete_df['class_name'] = complete_df['file_name'].map(lambda x: x.split('/')[-2])\n",
    "\n",
    "#### COMPROBACIÓN ####\n",
    "print(\"Número total de imágenes:\", len(complete_df))\n",
    "class_counts = complete_df['class_name'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dividimos \"complete_df\" en entrenamiento y test :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = complete_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_size = 0.2\n",
    "train_df_list = []\n",
    "test_df_list = []\n",
    "\n",
    "classes = complete_df['class_name'].unique()\n",
    "\n",
    "for class_name in classes:\n",
    "    class_subset = complete_df[complete_df['class_name'] == class_name]\n",
    "    \n",
    "    test_count = int(len(class_subset) * test_size)\n",
    "    \n",
    "    test_df_list.append(class_subset.iloc[:test_count])\n",
    "    train_df_list.append(class_subset.iloc[test_count:])\n",
    "\n",
    "train_df = pd.concat(train_df_list).reset_index(drop=True)\n",
    "test_df = pd.concat(test_df_list).reset_index(drop=True)\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#### COMPROBACIÓN ####\n",
    "print(f\"Nº de imágenes en train: {len(train_df)}\")\n",
    "print(train_df['class_name'].value_counts())\n",
    "print()\n",
    "print(f\"Nº de imágenes en test: {len(test_df)}\")\n",
    "print(test_df['class_name'].value_counts())\n",
    "print()\n",
    "print(\"Ejm. conjunto train:\")\n",
    "print(train_df.head(10))\n",
    "print()\n",
    "print(\"Ejm. conjunto test:\")\n",
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Imágenes de entrenamiento y test antes del pre-procesado :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_imagenes_antes_preprocesado(df):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        grouped = df.groupby('class_name')\n",
    "        for i, (class_name, group) in enumerate(grouped):\n",
    "            image_path = group['file_name'].iloc[0]\n",
    "            with zip_ref.open(image_path) as image_file:\n",
    "                image = Image.open(image_file).convert(\"RGB\")\n",
    "            plt.subplot(1, len(grouped), i + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(class_name)\n",
    "            plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "mostrar_imagenes_antes_preprocesado(train_df)\n",
    "mostrar_imagenes_antes_preprocesado(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pre-procesado de las imágenes :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (227, 227)\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device} disponible\")\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor()])\n",
    "\n",
    "def preprocesado_imagen(zip_ref, image_path):\n",
    "    with zip_ref.open(image_path) as image_file:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        image = transform(image)\n",
    "        return image\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    for index, row in train_df.iterrows():\n",
    "        image_tensor = preprocesado_imagen(zip_ref, row['file_name'])\n",
    "        X_train.append(image_tensor)\n",
    "        y_train.append(row['class_name'])\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        image_tensor = preprocesado_imagen(zip_ref, row['file_name'])\n",
    "        X_test.append(image_tensor)\n",
    "        y_test.append(row['class_name'])\n",
    "\n",
    "X_train = torch.stack(X_train)\n",
    "X_test = torch.stack(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_train = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "#### COMPROBACIÓN ####\n",
    "print(\"X_train:\", X_train.shape, X_train.device)\n",
    "print(\"y_train:\", y_train.shape, y_train.device)\n",
    "print(\"X_test:\", X_test.shape, X_test.device)\n",
    "print(\"y_test:\", y_test.shape, y_test.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Añadir conjunto de validación :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, X_train.device)\n",
    "print(\"y_train:\", y_train.shape, y_train.device)\n",
    "print(\"X_val:\", X_val.shape, X_val.device)\n",
    "print(\"y_val:\", y_val.shape, y_val.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Datasets y DataLoaders :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***AlexNet :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "#Sin pesos preentrenados\n",
    "alexnet_scratch = alexnet(weights=None)\n",
    "alexnet_scratch.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "#Con pesos preentrenados\n",
    "alexnet_weights = AlexNet_Weights.DEFAULT\n",
    "alexnet_pretrained = alexnet(weights=alexnet_weights)\n",
    "alexnet_pretrained.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "#Congelar capas\n",
    "for param in alexnet_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in alexnet_pretrained.classifier[6].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***ResNet50 :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "#Sin pesos preentrenados\n",
    "resnet50_scratch = resnet50(weights=None)\n",
    "resnet50_scratch.fc = nn.Linear(resnet50_scratch.fc.in_features, num_classes)\n",
    "\n",
    "#Con pesos preentrenados\n",
    "resnet50_weights = ResNet50_Weights.DEFAULT\n",
    "resnet50_pretrained = resnet50(weights=resnet50_weights)\n",
    "resnet50_pretrained.fc = nn.Linear(resnet50_pretrained.fc.in_features, num_classes)\n",
    "\n",
    "#Congelar capas\n",
    "for param in resnet50_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet50_pretrained.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***DenseNet121 :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "#Sin pesos preentrenados\n",
    "densenet121_scratch = densenet121(weights=None)\n",
    "densenet121_scratch.classifier = nn.Linear(densenet121_scratch.classifier.in_features, num_classes)\n",
    "\n",
    "#Con pesos preentrenados\n",
    "densenet121_weights = DenseNet121_Weights.DEFAULT\n",
    "densenet121_pretrained = densenet121(weights=densenet121_weights)\n",
    "densenet121_pretrained.classifier = nn.Linear(densenet121_pretrained.classifier.in_features, num_classes)\n",
    "\n",
    "#Congelar capas\n",
    "for param in densenet121_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in densenet121_pretrained.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***ConvNeXt Tiny :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "#Sin pesos preentrenados\n",
    "convnext_tiny_scratch = convnext_tiny(weights=None)\n",
    "in_features = convnext_tiny_scratch.classifier[2].in_features\n",
    "convnext_tiny_scratch.classifier[2] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "#Con pesos preentrenados\n",
    "convnext_tiny_weights = ConvNeXt_Tiny_Weights.DEFAULT\n",
    "convnext_tiny_pretrained = convnext_tiny(weights=convnext_tiny_weights)\n",
    "in_features = convnext_tiny_pretrained.classifier[2].in_features\n",
    "convnext_tiny_pretrained.classifier[2] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "#Congelar capas\n",
    "for param in convnext_tiny_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in convnext_tiny_pretrained.classifier[2].parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inicialización del modelo :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### AlexNet ####\n",
    "#modelo = alexnet_scratch.to(device)\n",
    "#modelo = alexnet_pretrained.to(device)\n",
    "\n",
    "#### ResNet50 ####\n",
    "#modelo = resnet50_scratch.to(device)\n",
    "#modelo = resnet50_pretrained.to(device)\n",
    "\n",
    "#### DenseNet121 ####\n",
    "#modelo = densenet121_scratch.to(device)\n",
    "#modelo = densenet121_pretrained.to(device)\n",
    "\n",
    "#### ConvNeXt Tiny ####\n",
    "#modelo = convnext_tiny_scratch.to(device)\n",
    "modelo = convnext_tiny_pretrained.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, modelo.parameters()), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Entrenamiento del modelo :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #Train\n",
    "    modelo.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = modelo(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        actual = labels\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == actual).sum().item()\n",
    "\n",
    "        progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "\n",
    "    #Val\n",
    "    modelo.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = modelo(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            actual = labels\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == actual).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * correct_val / total_val\n",
    "\n",
    "    train_accuracies.append(train_acc)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} ✅ | Train Acc: {train_acc:.2f}% - Train Loss: {avg_train_loss:.4f} | Val Acc: {val_acc:.2f}% - Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Resultados AlexNet :***\n",
    "\n",
    "***Representación 2 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :***\n",
    "\n",
    "***Representación 3 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :***\n",
    "\n",
    "---\n",
    "\n",
    "# ***Resultados ResNet50 :***\n",
    "\n",
    "***Representación 2 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :***\n",
    "\n",
    "***Representación 3 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :***\n",
    "\n",
    "---\n",
    "\n",
    "# ***Resultados DenseNet121 :***\n",
    "\n",
    "***Representación 2 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :***\n",
    "\n",
    "***Representación 3 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :***\n",
    "\n",
    "---\n",
    "\n",
    "# ***Resultados ConvNeXt Tiny :***\n",
    "\n",
    "***Representación 2 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :***\n",
    "\n",
    "***Representación 3 :***\n",
    "\n",
    "1. ***Longitud 8 :***  \n",
    "\n",
    "2. ***Longitud 18 :***   \n",
    "\n",
    "3. ***Longitud 28 :*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Graficar los resultados del entrenamiento :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "output_dir = './Resultados_entrenamiento/Representacion_2/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.savefig(f'{output_dir}ResNet50_8_con.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
